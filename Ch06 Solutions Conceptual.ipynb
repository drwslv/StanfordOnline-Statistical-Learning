{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 - Exercise Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "\n",
    "We perform best subset, forward stepwise, and backward stepwise selection on a single data set. For each approach, we obtain *p* + 1 models, containing 0, 1, 2, . . . , *p* predictors. Explain your answers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Which of the three models with *k* predictors has the smallest training RSS?\n",
    "\n",
    "Best subset selection will likely result in the smallest *training* RSS, because it considers all possible combinations $ {p \\choose k} = \\frac{p!}{k!(p-k)!}$, and chooses the one with the lowest RSS. Forward and backward selection iterate through $ 1 + p(p+1)/2 $ models and *could* result in selecting the same set of *k* predictors, but also could possibly miss the combinations that yield the lowest RSS in the selection algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Which of the three models with *k* predictors has the smallest test RSS?\n",
    "\n",
    "The test RSS can't be directly known, but can be estimated by the training RSS or by cross validation. Likely best subset selention will again select the model with the lowest RSS. To avoid overfitting, AIC, BIC, or adjusted $R^2$ can be used to evaluate the models in cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** True or False:\n",
    "\n",
    "- i. The predictors in the *k*-variable model identified by forward stepwise are a subset of the predictors in the (*k*+1)-variable model identified by forward stepwise selection.\n",
    "\n",
    "TRUE\n",
    "\n",
    "- ii. The predictors in the *k*-variable model identified by backward stepwise are a subset of the predictors in the (*k*+1)-variable model identified by backward stepwise selection.\n",
    "\n",
    "TRUE\n",
    "\n",
    "- iii. The predictors in the *k*-variable model identified by backward stepwise are a subset of the predictors in the (*k*+1)-variable model identified by forward stepwise selection.\n",
    "\n",
    "FALSE\n",
    "\n",
    "- iv. The predictors in the *k*-variable model identified by forward stepwise are a subset of the predictors in the (*k*+1)-variable model identified by backward stepwise selection.\n",
    "\n",
    "FALSE\n",
    "\n",
    "- v. The predictors in the *k*-variable model identified by best subset are a subset of the predictors in the (*k*+1)-variable model identified by best subset selection.\n",
    "\n",
    "FALSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\n",
    "\n",
    "For parts (a) through (c), indicate which of i. through iv. is correct. Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** The lasso, relative to least squares, is:\n",
    "\n",
    "- i. More flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.\n",
    "\n",
    "    - FALSE. The lasso is less flexible.\n",
    "\n",
    "- ii. More flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.\n",
    "\n",
    "    - FALSE. The lasso is less flexible.\n",
    "\n",
    "- iii. Less flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.\n",
    "\n",
    "    - TRUE. The lasso is less flexible, because it puts constraings on coefficient magnitudes, and in some cases shrinking them to zero. This means the model will have lower variance at the cost of potentially higher bias. But it is true that the lasso could provide higher accuracy if the increase in bias is less than the decrease in variance.\n",
    "\n",
    "- iv. Less flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.\n",
    "\n",
    "    - FALSE. Less flexible models will have lower variance at the cost of higher bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Repeat (a) for ridge regression relative to least squares.\n",
    "\n",
    "- i. FALSE. (Same as lasso.)\n",
    "\n",
    "- ii. FALSE. (Same as lasso.)\n",
    "\n",
    "- iii. TRUE. Ridge regression is less flexible, in that it puts constraints on coefficient size. It will have lower variance, and higher bias. (Same as lasso.)\n",
    "\n",
    "- iv. FALSE. (Same as lasso.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Repeat (a) for non-linear methods relative to least squares.\n",
    "\n",
    "- i. FALSE. More flexible models typically have lower bias and higher variance.\n",
    "\n",
    "- ii. TRUE. Non-linear methods are more flexible than least squares, and therefore have lower bias at the cost of higher variance. If the rise in variance is less than the reduction in bias, the model will provide higher accuracty in predictions.\n",
    "\n",
    "- iii. FALSE.\n",
    "\n",
    "- iv. FALSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3\n",
    "\n",
    "Suppose we estimate the regression coefficients in a linear regression model by minimizing\n",
    "\n",
    "$$ \\sum_{i=1}^n \\Biggl( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\Biggr) ^2 \\textrm{subject to} \\sum_{j=1}^p | \\beta_j | \\le s $$\n",
    "\n",
    "for a particular value of *s*. For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** As we increase *s* from 0, the training RSS will:\n",
    "\n",
    "- i. Increase initially, and then eventually start decreasing in an inverted U shape.\n",
    "\n",
    "- ii. Decrease initially, and then eventually start increasing in a U shape.\n",
    "\n",
    "- iii. Steadily increase.\n",
    "\n",
    "- iv. Steadily decrease.\n",
    "\n",
    "- v. Remain constant.\n",
    "\n",
    "At *s* = 0, *s* is binding and the expression is equivalent to a lasso regression with $ \\lambda = \\infty $, in which $ \\beta_j = 0 $ for all *j*. In this case, the prediction for any $y_i$ is the mean of the datas series, $ \\beta_0 $, which will likely have a fairly high RSS. As *s* increases, $ \\beta_j $ incrrease, and the RSS is reduced. This should happen steadily until the $ \\beta_j $ are equal to the OLS coefficients, at which point the RSS will saturate/stabilize. The answer is (iv.) Steadily decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Test RSS\n",
    "\n",
    "We have less certainty about the test RSS, but we can say that at *s* = 0 it will likely be high. With some ease of the contstraint, $ | \\beta_j | $ increase and the RSS is reduced, but with enough increase in *s* and $ | \\beta_j | $ we could possibly overfit the model, and the RSS begins to increases. The answer is (ii.) Decrease initially, and then eventually start increaseing in a U shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Variance (componenet of MSE)\n",
    "\n",
    "At *s* = 0, the variance of the model is at a minimum -- it will only be a function of $ \\beta_0 $ (which could vary from sample to sample, so the variance of the estimator is not zero). Any increase in *s*, and therefore $ | \\beta_j | $, will increase the variance, and continue to increase the variance until the constraint is no longer binding. The answer is (iii.) Steadily increase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Squared bias (componenet of MSE)\n",
    "\n",
    "The bias should decrease with more flexibility in the model, i.e. a model with smaller *s* and larger $ | \\beta_j | $. The answer is (iv.) Steadily decrease.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** Irreducible error (componenet of MSE)\n",
    "\n",
    "This is the lowest possible (Bayesian) error, and is constant. The answer is (v.) remain constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4\n",
    "\n",
    "Suppose we estimate the regression coefficients in a linear regression model by minimizing\n",
    "\n",
    "$$ \\sum_{i=1}^n \\Biggl( y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} \\Biggr) ^2 + \\lambda \\sum_{j=1}^p \\beta_j^2 $$\n",
    "\n",
    "for a particular value of $ \\lambda $. For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** As we increase $ \\lambda $ from 0, the training RSS will:\n",
    "\n",
    "- i. Increase initially, and then eventually start decreasing in an inverted U shape.\n",
    "\n",
    "- ii. Decrease initially, and then eventually start increasing in a U shape.\n",
    "\n",
    "- iii. Steadily increase.\n",
    "\n",
    "- iv. Steadily decrease.\n",
    "\n",
    "- v. Remain constant.\n",
    "\n",
    "At $ \\lambda = 0 $, minimizing the expression is equivalent to OLS, and RSS is at a minimum. As $ \\lambda $ increases, a constraint is placed on the size of the $ \\beta_j $, the fit of the model becomes less flexible, and the RSS increases. The answer is (iii.) Steadily increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Test RSS\n",
    "\n",
    "Increasing $ \\lambda $ from 0 (OLS equivalent) likely improves performance, reducing test RSS. But increasing $ \\lambda $ too high leads to increasing test RSS again. The answer is (ii.) Decrease initially, and then eventually start increasing in a U shape.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Variance (componenet of MSE)\n",
    "\n",
    "Increasing $ \\lambda $ from 0 (OLS equivalent) reduces variance until $ \\lambda $ is large enough to reduce $ \\beta_j $ to zero, at which point the variance is at a minimum. The answer is (iv.) Steadily decrease.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Squared bias (componenet of MSE)\n",
    "\n",
    "Increasing $ \\lambda $ from 0 (OLS equivalent), the bias will increase with less flexibility in the model. The answer is (iii.) Steadily increase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** Irreducible error (componenet of MSE)\n",
    "\n",
    "This is the lowest possible (Bayesian) error, and is constant. The answer is (v.) remain constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5\n",
    "\n",
    "It is well-known that ridge regression tends to give similar coefficient values to correlated variables, whereas the lasso may give quite different coefficient values to correlated variables. We will now explore this property in a very simple setting.\n",
    "\n",
    "Suppose that $ n = 2, p = 2, x_{11} = x_{12}, x_{21} = x_{22} $. Furthermore, suppose that $ y_1 + y_2 = 0 $ and $ x_{11} + x_{21} = 0 $ and $ x_{12} + x_{22} = 0 $, so that the estimate for the intercept in a least squares, ridge regression, or lasso model is zero: $ \\hat{\\beta}_0 = 0 $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**\n",
    "\n",
    "Write out the ridge regression optimization problem in this setting.\n",
    "\n",
    "$$ \\hat{\\beta} = \\argmin_{\\beta} \\Biggl( \\sum_{j=1}^p (y_j - X \\beta_j)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2 \\Biggr) $$ \n",
    "\n",
    "$$ \\hat{\\beta} = \\argmin_{\\beta} \\Biggl( (y_1 - [x_{11} x_{12}] [\\beta_1 \\beta_2]^T)^2 + (y_2 - [x_{21} x_{22}] [\\beta_1 \\beta_2]^T )^2 + \\lambda (  \\beta_1^2 + \\beta_2^2) \\Biggr) $$ \n",
    "\n",
    "$$ \\hat{\\beta} = \\argmin_{\\beta} \\Biggl( (y_1 - \\beta_1 x_{11} - \\beta_2 x_{12})^2 + (y_2 - \\beta_1 x_{21} - \\beta_2x_{22})^2 + \\lambda \\beta_1^2 + \\lambda \\beta_2^2 \\Biggr) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the fact that $ x_{11} = x_{12} = x_1 $ and $ x_{21} = x_{22} = x_2 $ and $ y_1 + y_2 = 0 $\n",
    "\n",
    "$$ \\hat{\\beta} = \\argmin_{\\beta} \\Biggl( (y_1 - x_1 ( \\beta_1 + \\beta_2 ) )^2 + (y_2 - x_2 (\\beta_1 +\\beta_2) )^2 + \\lambda \\beta_1^2 + \\lambda \\beta_2^2 \\Biggr) $$\n",
    "\n",
    "$$ \\hat{\\beta} = \\argmin_{\\beta} \\Biggl( y_1^2 + x_1^2 ( \\beta_1 + \\beta_2 )^2 - 2 y_1 x_1 (\\beta_1 + \\beta_2)  + y_2^2 + x_2^2 (\\beta_1 +\\beta_2)^2 -2 y_2 x_2 (\\beta_1 +\\beta_2) + \\lambda \\beta_1^2 + \\lambda \\beta_2^2 \\Biggr) $$\n",
    "\n",
    "$$ \\hat{\\beta} = \\argmin_{\\beta} \\Biggl( y_1^2 + x_1^2 ( \\beta_1^2 + \\beta_2^2 + 2 \\beta_1 \\beta_2 ) - 2 y_1 x_1 (\\beta_1 + \\beta_2)  + y_2^2 + x_2^2 (\\beta_1^2 + \\beta_2^2 + 2 \\beta_1 \\beta_2) -2 y_2 x_2 (\\beta_1 +\\beta_2) + \\lambda \\beta_1^2 + \\lambda \\beta_2^2 \\Biggr) $$\n",
    "\n",
    "$$ \\hat{\\beta} = \\argmin_{\\beta} \\Biggl( y_1^2 + ( \\beta_1^2 x_1^2 + \\beta_2^2 x_1^2 + 2 \\beta_1 \\beta_2 x_1^2 ) - 2 y_1 x_1 \\beta_1 - 2 y_1 x_1  \\beta_2  + y_2^2 + (\\beta_1^2 x_2^2 + \\beta_2^2 x_2^2 + 2 \\beta_1 \\beta_2 x_2^2) -2 y_2 x_2 \\beta_1 -2 y_2 x_2 \\beta_2 + \\lambda \\beta_1^2 + \\lambda \\beta_2^2 \\Biggr) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the partial derivative with respect to $ \\beta_1 $\n",
    "\n",
    "$$ 0 = \\frac{\\partial}{\\partial \\beta_1} \\Biggl( y_1^2 + ( \\beta_1^2 x_1^2 + \\beta_2^2 x_1^2 + 2 \\beta_1 \\beta_2 x_1^2 ) - 2 y_1 x_1 \\beta_1 - 2 y_1 x_1  \\beta_2  + y_2^2 + (\\beta_1^2 x_2^2 + \\beta_2^2 x_2^2 + 2 \\beta_1 \\beta_2 x_2^2) -2 y_2 x_2 \\beta_1 -2 y_2 x_2 \\beta_2 + \\lambda \\beta_1^2 + \\lambda \\beta_2^2 \\Biggr) $$\n",
    "\n",
    "$$ 0 =  ( 2 \\beta_1 x_1^2 + 2 \\beta_2 x_1^2 ) - 2 y_1 x_1 + ( 2 \\beta_1 x_2^2 + 2 \\beta_2 x_2^2) - 2 y_2 x_2 + 2 \\lambda \\beta_1   $$\n",
    "\n",
    "$$ 0 =  \\beta_1 (x_1^2 + x_2^2 + \\lambda) + \\beta_2 (x_1^2 + x_2^2 ) - y_1 x_1  - y_2 x_2   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the partial derivative with respect to $ \\beta_2 $\n",
    "\n",
    "$$ 0 = \\frac{\\partial}{\\partial \\beta_2} \\Biggl( y_1^2 + ( \\beta_1^2 x_1^2 + \\beta_2^2 x_1^2 + 2 \\beta_1 \\beta_2 x_1^2 ) - 2 y_1 x_1 \\beta_1 - 2 y_1 x_1  \\beta_2  + y_2^2 + (\\beta_1^2 x_2^2 + \\beta_2^2 x_2^2 + 2 \\beta_1 \\beta_2 x_2^2) -2 y_2 x_2 \\beta_1 -2 y_2 x_2 \\beta_2 + \\lambda \\beta_1^2 + \\lambda \\beta_2^2 \\Biggr) $$\n",
    "\n",
    "$$ 0 =  2 \\beta_2 x_1^2 + 2 \\beta_1 x_1^2 - 2 y_1 x_1 + 2 \\beta_2 x_2^2 + 2 \\beta_1 x_2^2 - 2 y_2 x_2 + 2 \\lambda \\beta_2  $$\n",
    "\n",
    "$$ 0 =  \\beta_2 ( x_1^2 + x_2^2 + \\lambda) + \\beta_1 ( x_1^2 + x_2^2 ) - y_1 x_1 - y_2 x_2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization system is\n",
    "\n",
    "$$ 0 =  \\beta_1 (x_1^2 + x_2^2 + \\lambda) + \\beta_2 (x_1^2 + x_2^2 ) - y_1 x_1  - y_2 x_2   $$\n",
    "\n",
    "$$ 0 =  \\beta_2 ( x_1^2 + x_2^2 + \\lambda) + \\beta_1 ( x_1^2 + x_2^2 ) - y_1 x_1 - y_2 x_2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substitute\n",
    "\n",
    "$$ (x_1^2 + x_2^2) = (x_1^2 - x_2^2) + 2 x_2^2 $$\n",
    "\n",
    "And can factor\n",
    "\n",
    "$$ (x_1^2 - x_2^2) $$\n",
    "\n",
    "$$ (x_1 + x_2) (x_1 - x_2) $$\n",
    "\n",
    "Using the fact that $ x_1 + x_2 = 0 $, we know that $ (x_1^2 - x_2^2) = 0 $, and so in this special case\n",
    "\n",
    "$$ (x_1^2 + x_2^2) = 2 x_2^2 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rewdrite the optimization system as\n",
    "\n",
    "$$ 0 =  \\beta_1 (2x_2^2 + \\lambda) + \\beta_2 (2x_2^2 ) - y_1 x_1  - y_2 x_2   $$\n",
    "\n",
    "$$ 0 =  \\beta_2 (2x_2^2 + \\lambda) + \\beta_1 ( 2x_2^2 ) - y_1 x_1 - y_2 x_2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Argue that in this setting, the ridge coefficient estimates satisfy $ \\hat{\\beta}_1 = \\hat{\\beta}_2 $.\n",
    "\n",
    "Write the system as\n",
    "\n",
    "$$ 0 =  a \\beta_1 + b \\beta_2 + c $$\n",
    "\n",
    "$$ 0 =  a \\beta_2 + b \\beta_1 + c $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the two sides equal to one another\n",
    "\n",
    "$$  a \\beta_1 + b \\beta_2 = a \\beta_2 + b \\beta_1  $$\n",
    "\n",
    "$$  a \\beta_1 - b \\beta_1  = a \\beta_2  - b \\beta_2 $$\n",
    "\n",
    "$$  \\beta_1 (a - b )  = \\beta_2 (a - b) $$\n",
    "\n",
    "$$  \\beta_1  = \\beta_2 $$\n",
    "\n",
    "\n",
    "SO, there are many solutions, as longa as $ \\beta_1  = \\beta_2 $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algibraically** \n",
    "\n",
    "Solve 2nd equation for $ \\beta_1 $\n",
    "\n",
    "$$  b \\beta_1 = - a \\beta_2 - c $$\n",
    "\n",
    "$$  \\beta_1 = - (a/b) \\beta_2 - c/b $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substitute $ \\beta_1 $ into 2st equation\n",
    "\n",
    "$$ 0 =  a \\biggl[ (a/b) \\beta_2 - c/b \\biggr] + b \\beta_2 + c $$\n",
    "\n",
    "$$ 0 =  (aa/b) \\beta_2 - c/b + b \\beta_2 + c $$\n",
    "\n",
    "$$ (aa/b) \\beta_2 + b \\beta_2  = c/b - c $$\n",
    "\n",
    "$$  \\beta_2 \\biggl[  (aa/b) + b \\biggr]  = c/b - c $$\n",
    "\n",
    "$$  \\beta_2  = \\biggl[ c/b - c \\biggr] / \\biggl[  (aa/b) + b \\biggr] $$\n",
    "\n",
    "Solve for $ \\beta_1 $\n",
    "\n",
    "$$  \\beta_1 = - (a/b) \\beta_2 - c/b $$\n",
    "\n",
    "$$  \\beta_1 = - (a/b)      \\biggl[ c/b - c \\biggr] / \\biggl[  (aa/b) + b \\biggr]       - c/b $$\n",
    "\n",
    "$$  \\beta_1 =     \\biggl[ - ac/bb + ac/b \\biggr] / \\biggl[  (aa/b) + b \\biggr]       - c/b $$\n",
    "\n",
    "$$  \\beta_1 =     \\biggl[ - ac/b + ac \\biggr] / \\biggl[  (aa) + bb \\biggr]       - c/b $$\n",
    "\n",
    "(What a mess...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Write out the lasso optimization problem in this setting.\n",
    "\n",
    "$$ \\hat{\\beta} = \\argmin_{\\beta} \\Biggl( \\sum_{j=1}^p (y_j - X \\beta_j)^2 + \\lambda \\sum_{j=1}^p | \\beta_j | \\Biggr) $$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the fact that $ x_{11} = x_{12} = x_1 $ and $ x_{21} = x_{22} = x_2 $ and $ y_1 + y_2 = 0 $\n",
    "\n",
    "$$ \\hat{\\beta} = \\argmin_{\\beta} \\Biggl( y_1^2 + ( \\beta_1^2 x_1^2 + \\beta_2^2 x_1^2 + 2 \\beta_1 \\beta_2 x_1^2 ) - 2 y_1 x_1 \\beta_1 - 2 y_1 x_1  \\beta_2  + y_2^2 + (\\beta_1^2 x_2^2 + \\beta_2^2 x_2^2 + 2 \\beta_1 \\beta_2 x_2^2) -2 y_2 x_2 \\beta_1 -2 y_2 x_2 \\beta_2 + \\lambda | \\beta_1 | + \\lambda | \\beta_2 | \\Biggr) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the partial derivative with respect to $ \\beta_1 $\n",
    "\n",
    "$$ 0 = \\frac{\\partial}{\\partial \\beta_1} \\Biggl( y_1^2 + ( \\beta_1^2 x_1^2 + \\beta_2^2 x_1^2 + 2 \\beta_1 \\beta_2 x_1^2 ) - 2 y_1 x_1 \\beta_1 - 2 y_1 x_1  \\beta_2  + y_2^2 + (\\beta_1^2 x_2^2 + \\beta_2^2 x_2^2 + 2 \\beta_1 \\beta_2 x_2^2) -2 y_2 x_2 \\beta_1 -2 y_2 x_2 \\beta_2 + \\lambda | \\beta_1 |+ \\lambda | \\beta_2 | \\Biggr) $$\n",
    "\n",
    "$$ 0 =  ( 2 \\beta_1 x_1^2 + 2 \\beta_2 x_1^2 ) - 2 y_1 x_1 + ( 2 \\beta_1 x_2^2 + 2 \\beta_2 x_2^2) - 2 y_2 x_2 + \\lambda  $$\n",
    "\n",
    "$$ 0 =  \\beta_1 (x_1^2 + x_2^2 ) + \\beta_2 (x_1^2 + x_2^2 ) - y_1 x_1  - y_2 x_2 + \\lambda /2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the partial derivative with respect to $ \\beta_2 $\n",
    "\n",
    "$$ 0 = \\frac{\\partial}{\\partial \\beta_2} \\Biggl( y_1^2 + ( \\beta_1^2 x_1^2 + \\beta_2^2 x_1^2 + 2 \\beta_1 \\beta_2 x_1^2 ) - 2 y_1 x_1 \\beta_1 - 2 y_1 x_1  \\beta_2  + y_2^2 + (\\beta_1^2 x_2^2 + \\beta_2^2 x_2^2 + 2 \\beta_1 \\beta_2 x_2^2) -2 y_2 x_2 \\beta_1 -2 y_2 x_2 \\beta_2 + \\lambda | \\beta_1 | + \\lambda | \\beta_2 | \\Biggr) $$\n",
    "\n",
    "$$ 0 =  2 \\beta_2 x_1^2 + 2 \\beta_1 x_1^2 - 2 y_1 x_1 + 2 \\beta_2 x_2^2 + 2 \\beta_1 x_2^2 - 2 y_2 x_2 + \\lambda   $$\n",
    "\n",
    "$$ 0 =  \\beta_2 ( x_1^2 + x_2^2 ) + \\beta_1 ( x_1^2 + x_2^2 ) - y_1 x_1 - y_2 x_2 + \\lambda /2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization system is\n",
    "\n",
    "$$ 0 =  \\beta_1 (x_1^2 + x_2^2 ) + \\beta_2 (x_1^2 + x_2^2) - y_1 x_1  - y_2 x_2 + \\lambda /2 $$\n",
    "\n",
    "$$ 0 =  \\beta_2 ( x_1^2 + x_2^2 ) + \\beta_1 ( x_1^2 + x_2^2) - y_1 x_1 - y_2 x_2 + \\lambda /2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we can rewrite, as before\n",
    "\n",
    "$$ 0 =  \\beta_1 ( 2 x_2^2 ) + \\beta_2 ( 2 x_2^2 ) - y_1 x_1  - y_2 x_2 + \\lambda /2 $$\n",
    "\n",
    "$$ 0 =  \\beta_2 ( 2 x_2^2 ) + \\beta_1 ( 2 x_2^2 ) - y_1 x_1 - y_2 x_2 + \\lambda /2$$\n",
    "\n",
    "In this case, the minimizatino system yields two identical equations, so we cannot solve the system (the two lines run in parallel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Argue that in this setting, the lasso coefficients $ \\hat{\\beta}_1 $ and $ \\hat{\\beta}_2 $ are not unique -- in other words, there are many possible solutions to the optimization problem in (c). Describe these solutions.\n",
    "\n",
    "\n",
    "Write the system as\n",
    "\n",
    "$$ 0 =  a \\beta_1 + a \\beta_2 + c $$\n",
    "\n",
    "$$ 0 =  a \\beta_2 + a \\beta_1 + c $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the two sides equal to one another\n",
    "\n",
    "$$ a \\beta_1 + a \\beta_2 = a \\beta_1 + a \\beta_2 $$\n",
    "\n",
    "$$ \\beta_1 + \\beta_2 = \\beta_1 + \\beta_2 $$\n",
    "\n",
    "$$ \\beta_1 - \\beta_1 = \\beta_2 - \\beta_2 $$\n",
    "\n",
    "The only definite statement we can make is that sum to a constant, that is, they differe by a fixed amount\n",
    "\n",
    "$$ \\beta_1 + \\beta_2 = -c/a =  (-) \\frac{- y_1 x_1  - y_2 x_2 + \\lambda /2}{x_1^2 + x_2^2} $$\n",
    "\n",
    "$$ \\beta_1 + \\beta_2  =  \\frac{y_1 x_1 + y_2 x_2 - \\lambda /2}{x_1^2 + x_2^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6\n",
    "\n",
    "We will now explore (6.12) and (6.13) further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Consider (6.12) with $ p = 1 $. For some choice of $ y_1 $ and $ \\lambda > 0 $, plot (6.12) as a function of $ \\beta_1 $. Your plot should confirm that (6.12) is solved by (6.14).\n",
    "\n",
    "$$ \\sum_{j=1}^p (y_j - \\beta_j)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2 $$\n",
    "\n",
    "with $ n = p = 1 $\n",
    "\n",
    "$$ (y_1 - \\beta_1)^2 + \\lambda \\beta_1^2 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "beta = np.arange(100)\n",
    "y = 50\n",
    "lam = 1\n",
    "rss_ridge = (y - beta)**2 + lam * beta**2\n",
    "\n",
    "fig, ax = subplots(figsize=(5,5))\n",
    "ax.scatter(beta, rss_ridge);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min1 = rss_ridge.argmin()\n",
    "\n",
    "min2 = y/(1 + lam)\n",
    "\n",
    "np.allclose(min1, min2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Consider (6.13) with $p = 1$. For some choice of $ y_1 $ and $ \\lambda > 0 $, plot (6.13) as a function of $ \\beta_1 $. Your plot should confirm that (6.13) is solved by (6.15).\n",
    "\n",
    "\n",
    "$$ \\sum_{j=1}^p (y_j - \\beta_j)^2 + \\lambda \\sum_{j=1}^p | \\beta_j | $$\n",
    "\n",
    "with $ n = p = 1 $\n",
    "\n",
    "$$ (y_1 - \\beta_1)^2 + \\lambda | \\beta_1 | $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "beta = np.arange(100)\n",
    "y = 50\n",
    "lam = 1\n",
    "rss_lasso = (y - beta)**2 + lam * np.abs(beta)\n",
    "\n",
    "fig, ax = subplots(figsize=(5,5))\n",
    "ax.scatter(beta, rss_lasso);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min1 = rss_lasso.argmin()\n",
    "\n",
    "if (y > lam/2):\n",
    "    min2 = y - lam/2\n",
    "elif (y < -lam/2):\n",
    "    min2 = y + lam/2\n",
    "else:\n",
    "    min2 = 0\n",
    "\n",
    "np.allclose(min1, min2, atol= 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7\n",
    "\n",
    "We will now derive the Bayesian connection to the lasso and ridge regression discussed in Section 6.2.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Suppose that $ y_i = \\beta_0 + \\sum_{j=1}^p x_{ij} \\beta_j + \\epsilon_i $ where $ \\epsilon_1, \\ldots , \\epsilon_n $ are independent and identically distributed from a $N(0, \\sigma^2) $ distribution. Write out the likelihood for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood function for the data in a simple linear model, where $ \\beta = \\beta_0, \\beta_1, \\ldots, \\beta_j$, is\n",
    "\n",
    "$$ f(Y | X, \\beta) = L(\\beta, \\sigma^2 | Y, X) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y_i - \\beta_0 - \\sum_{j=1}^p x_{ij} \\beta_j  )^2}{2\\sigma^2} \\right) $$\n",
    "\n",
    "which is a product of normal distributions. This is the case because the only variability in the model comes from the error term $ \\epsilon $, which is i.i.d. with a normal distribution by assumption. Therefor, the likelihood function is the product of a series of normal distributions, each centered around $ y_i $ with mean given by the linear model itself: $ \\beta_0 + \\sum_{j=1}^p x_{ij} \\beta_j $.\n",
    "\n",
    "Simplifiying\n",
    "\n",
    "$$ f(Y | X, \\beta) = \\frac{1}{(\\sqrt{2\\pi\\sigma^2})^n} \\prod_{i=1}^{n} \\exp\\left( -\\frac{(y_i - \\beta_0 - \\sum_{j=1}^p x_{ij} \\beta_j  )^2}{2\\sigma^2} \\right) $$\n",
    "\n",
    "$$ f(Y | X, \\beta) = \\frac{1}{(2\\pi\\sigma^2)^{n/2}} \\exp\\left( \\sum_{i=1}^{n}  -\\frac{(y_i - \\beta_0 - \\sum_{j=1}^p x_{ij} \\beta_j  )^2}{2\\sigma^2} \\right) $$\n",
    "\n",
    "$$ f(Y | X, \\beta) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n}  (y_i - \\beta_0 - \\sum_{j=1}^p x_{ij} \\beta_j  )^2 \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Assume the following prior for $ \\beta $: $\\beta_1, \\ldots, \\beta_p $ are independent and identically distributed according to a double-exponential distribution with mean 0 and common scale parameter $ b $: i.e.\n",
    "\n",
    "$$ p(\\beta) = \\frac{1}{2b} \\exp(−|\\beta|/b) $$\n",
    "\n",
    "Write out the posterior for $ \\beta $ in this setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume $ p(\\beta) $ takes the form\n",
    "\n",
    "$$ p(\\beta) = \\prod_{j=1}^p g(\\beta_j) $$\n",
    "\n",
    "where $ g(\\cdot) $ is the distribution of an individual $ \\beta $. If individual $ \\beta s $ are distributed double-exponential, the prior becomes\n",
    "\n",
    "$$ p(\\beta) = \\prod_{j=1}^p \\frac{1}{2b} \\exp(−|\\beta|/b)  $$\n",
    "\n",
    "$$ p(\\beta) = \\frac{1}{(2b)^p}  \\exp( \\sum_{j=1}^p  −|\\beta_j|/b)  $$\n",
    "\n",
    "$$ p(\\beta) = (2b)^{-p} \\exp( -\\frac{1}{pb} \\sum_{j=1}^p |\\beta_j|)  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior distribution takes the form\n",
    "\n",
    "$$ p(\\beta | X, Y) \\propto f(Y | X, \\beta) p(\\beta | X) = f(Y | X, \\beta) p(\\beta) $$\n",
    "\n",
    "Technically\n",
    "\n",
    "$$ p(\\beta | X, Y) = \\frac{f(Y | X, \\beta) p(\\beta | X)}{P(Y | X)}  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting\n",
    "\n",
    "$$ p(\\beta | X, Y) \\propto f(Y | X, \\beta) p(\\beta) =  (2\\pi\\sigma^2)^{-n/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n}  (y_i - \\beta_0 - \\sum_{j=1}^p x_{ij} \\beta_j  )^2 \\right) (2b)^{-p} \\exp( -\\frac{1}{pb} \\sum_{j=1}^p |\\beta_j|)  $$\n",
    "\n",
    "Taking logs\n",
    "\n",
    "$$ \\ln p(\\beta | X, Y) = -n/2 \\ln (2\\pi\\sigma^2) + \\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n}  (y_i - \\beta_0 - \\sum_{j=1}^p x_{ij} \\beta_j  )^2 \\right) -p \\ln (2b) -\\frac{1}{pb} \\sum_{j=1}^p |\\beta_j| $$\n",
    "\n",
    "$$ \\ln p(\\beta | X, Y) = \\Biggl[ -n/2 \\ln (2\\pi\\sigma^2) -p \\ln (2b) \\Biggr]  -\\frac{1}{2\\sigma^2} \\Biggl[ \\sum_{i=1}^{n}  (y_i - \\beta_0 - \\sum_{j=1}^p x_{ij} \\beta_j  )^2 \\Biggr]  -  \\Biggl[ \\frac{1}{pb}  \\sum_{j=1}^p |\\beta_j| \\Biggr] $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Argue that the lasso estimate is the *mode* for $ \\beta $ under this posterior distribution.\n",
    "\n",
    "Maximization of this log-likelihood function is equal to minimization of its opposite. The first term is a constant, and does not influence maximization. Maximization of the 2nd term is minimizatino of the RSS term in the lasso. Maximization of the 3rd term is minimization of the penalty term in the lasso, in which $ 1/pb = \\lambda $.\n",
    "\n",
    "In this case, the $ \\beta $ posterior is the product of a normal and double-exponential distribution, and in this case the mean is not equal to the mode. Note that $ \\lambda $ is not a mere constant, but rather a function of the number of parameters, $ p $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Now assume the following prior for $ \\beta $: $\\beta_1, \\ldots, \\beta_p $  are independent and identically distributed according to a normal distribution with mean zero and variance *c*. Write out the posterior for $ \\beta $ in this setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now $ p(\\beta) $ takes the general form\n",
    "\n",
    "$$ p(\\beta) = \\prod_{j=1}^p g(\\beta_j) $$\n",
    "\n",
    "where $ g(\\cdot) $ is the normal distribution. So $ p(\\beta) $ is\n",
    "\n",
    "$$ p(\\beta) = \\prod_{j=1}^p \\frac{1}{\\sqrt{2\\pi c}} \\exp\\left( -\\frac{\\beta_j^2}{2 c} \\right) $$\n",
    "\n",
    "$$ p(\\beta) = (2 \\pi c)^{-p/2} \\prod_{j=1}^p \\exp\\left( -\\frac{\\beta_j^2}{2 c} \\right) $$\n",
    "\n",
    "Taking logs\n",
    "\n",
    "$$ \\ln p(\\beta) = -\\frac{p}{2} \\ln (2 \\pi c) +  \\sum_{j=1}^p \\left( -\\frac{\\beta_j^2}{2 c} \\right) $$\n",
    "\n",
    "$$ \\ln p(\\beta) = -\\frac{p}{2} \\ln (2 \\pi c) - \\frac{1}{2c} \\sum_{j=1}^p \\beta_j^2 $$\n",
    "\n",
    "Adding this to $ \\ln f(Y | \\beta, X ) $\n",
    "\n",
    "$$ \\ln p(\\beta | X, Y) = -n/2 \\ln (2\\pi\\sigma^2) + \\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n}  (y_i - \\beta_0 - \\sum_{j=1}^p x_{ij} \\beta_j  )^2 \\right)  -\\frac{p}{2} \\ln (2 \\pi c) - \\frac{1}{2c} \\sum_{j=1}^p \\beta_j^2 $$\n",
    "\n",
    "$$ \\ln p(\\beta | X, Y) = \\Biggl[ -n/2 \\ln (2\\pi\\sigma^2) -\\frac{p}{2} \\ln (2 \\pi c) \\Biggr]  -\\frac{1}{2\\sigma^2} \\Biggl[ \\sum_{i=1}^{n}  (y_i - \\beta_0 - \\sum_{j=1}^p x_{ij} \\beta_j  )^2 \\Biggr]  - \\frac{1}{2c} \\Biggl[ \\sum_{j=1}^p \\beta_j^2 \\Biggr] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** Argue that the ridge regression estimate is both the *mode* and the *mean* for $ \\beta $ under this posterior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, maximization of this log-likelihood function is equal to minimization of its opposite. The first term is a constant, and does not influence maximization. Maximization of the 2nd term is minimizatino of the RSS term in the ridge. Maximization of the 3rd term is minimization of the penalty term in the ridge, in which $ 1/2c = \\lambda $.\n",
    "\n",
    "The ridge regression estimates are equal to both the mean and mode of the $ \\beta $ posterior (via maximum likelihood) becuase the posterior if a function of multiple normal distributions, in which the mean is equal to the mode."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "islp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
