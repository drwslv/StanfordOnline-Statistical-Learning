{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10 - Excercise Solutions - Conceptual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "\n",
    "Consider a neural network with two hidden layers: *p* = 4 input units, 2 units in the first hidden layer, 3 units in the second hidden layer, and a single output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Draw a picture of the network, similar to Figures 10.1 or 10.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](Ch10_NN_4x2x3x1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Write out an expression for f(X), assuming ReLU activation functions. Be as explicit as you can!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first hidden layer:\n",
    "\n",
    "$$ A_k^{(1)} = h_k^{(1)} (X) $$\n",
    "\n",
    "$$ A_k^{(1)} = g \\left( w_{k0}^{(1)} + \\sum_{j=1}^{4} w_{kj}^{(1)} X_j \\right) $$\n",
    "\n",
    "for $ k = 1, 2 $\n",
    "\n",
    "$ \\mathbf{W_1} $ has dimensions: (1 + 4) $ \\times $ 2, i.e. **10** elemenets\n",
    "\n",
    "[`1 + seq_length` $\\times$ `num_features_l1`]\n",
    "\n",
    "The second hidden layer:\n",
    "\n",
    "$$ A_l^{(2)} = h_l^{(2)} (X) $$\n",
    "\n",
    "$$ A_l^{(2)} = g \\left( w_{l0}^{(2)} + \\sum_{l=1}^{2} w_{lk}^{(2)} A_k^{(1)} \\right) $$\n",
    "\n",
    "for $ l = 1, 2, 3 $ (differentiated from $ k $ for clarity)\n",
    "\n",
    "$ \\mathbf{W_2} $ has dimensions: (1 + 2) $ \\times $ 3, i.e. **9** elements\n",
    "\n",
    "[`1 + num_features_l1` $\\times$ `num_features_l2`]\n",
    "\n",
    "$g(z)$ is the ReLU (rectified linear unit) function, defined as:\n",
    "\n",
    "$$\n",
    "g(z) =\n",
    "\\begin{cases} \n",
    "      0 & \\text{if } z < 0 \\\\\n",
    "      z & \\text{if } z \\geq 0 \n",
    "   \\end{cases}\n",
    "$$\n",
    "\n",
    "The output layer:\n",
    "\n",
    "$$ f_0(X) = \\beta_0 + \\sum_{l=1}^{3} \\beta_l h_l^{(2)}(X) $$\n",
    "\n",
    "$ \\mathbf{B} $ has dimensions: (1 + 3) $ \\times $ 1, i.e. **4** elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the layers:\n",
    "\n",
    "$$ f_0(X) = \\beta_0 + \\sum_{l=1}^{3} \\beta_l \\; \\; g \\left[ w_{l0}^{(2)} + \\sum_{l=1}^{2} w_{lk}^{(2)} A_k^{(1)} \\right] $$\n",
    "\n",
    "$$ f_0(X) = \\beta_0 + \\sum_{l=1}^{3} \\beta_l \\; \\; g \\left[ w_{l0}^{(2)} + \\sum_{l=1}^{2} w_{lk}^{(2)} \\; g \\left( w_{k0}^{(1)} + \\sum_{j=1}^{4} w_{kj}^{(1)} X_j \\right)   \\right] $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Now plug in some values for the coefficients and write out the value of f(X)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.1607552059888944\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.random.uniform(-10, 10, 4)\n",
    "\n",
    "np.random.seed(1)\n",
    "A1k1 = np.random.uniform(-1, 0, 4) # for k = 1\n",
    "A11 = np.random.uniform(-1, 1) + X.dot(A1k1) # 1 + 4 parameters\n",
    "\n",
    "np.random.seed(2)\n",
    "A1k2 = np.random.uniform(-1, 0, 4) # for k = 2\n",
    "A12 = np.random.uniform(-1, 1) + X.dot(A1k1) # 1 + 4 parameters\n",
    "\n",
    "A1 = abs(np.array([A11, A12])) # ReLU layer\n",
    "\n",
    "np.random.seed(3)\n",
    "A2k1 = np.random.uniform(-1, 0, 2) # for l = 1\n",
    "A21 = np.random.uniform(-1, 1) + A1.dot(A2k1) # 1 + 2 parameters\n",
    "\n",
    "np.random.seed(4)\n",
    "A2k2 = np.random.uniform(-1, 0, 2) # for l = 2\n",
    "A22 = np.random.uniform(-1, 1) + A1.dot(A2k2) # 1 + 2 parameters\n",
    "\n",
    "np.random.seed(5)\n",
    "A2k3 = np.random.uniform(-1, 0, 2) # for l = 3\n",
    "A23 = np.random.uniform(-1, 1) + A1.dot(A2k3) # 1 + 2 parameters\n",
    "\n",
    "A2 = abs(np.array([A21, A22, A23])) # ReLU layer\n",
    "\n",
    "np.random.seed(6)\n",
    "B1 = np.random.uniform(-1, 0, 3) # length 3 for size of prev layer\n",
    "B = np.random.uniform(-1, 1) + A2.dot(B1) # 1 + 3 parameters\n",
    "# No ReLU for output layer\n",
    "print(B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** How many parameters are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 23 parameters in total:\n",
    "\n",
    "* First hidden layer: (1 + 4) $ \\times $ 2, i.e. **10** elemenets\n",
    "\n",
    "* Second hidden layer: (1 + 2) $ \\times $ 3, i.e. **9** elements\n",
    "\n",
    "* Output layer: (1 + 3) $ \\times $ 1, i.e. **4** elements\n",
    "\n",
    "10 + 9 + 4 = 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n",
    "Consider the *softmax* function in (10.13) (see also (4.13) on page 145) for modeling multinomial probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** In (10.13), show that if we add a constant $c$ to each of the $Z_l$, then the probability is unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original formula:\n",
    "\n",
    "$$ f_m(X) = \\mathrm{Pr}(Y = m | X) = \\frac{e^{Z_m}}{\\sum_{l=0}^{9} e^{Z_l} }$$\n",
    "\n",
    "Adding $c$ to each $Z_L$\n",
    "\n",
    "$$ = \\frac{e^{Z_m + c}}{\\sum_{l=0}^{9} e^{Z_l + c} }$$\n",
    "\n",
    "$$ = \\frac{e^{Z_m} e^{c}}{\\sum_{l=0}^{9} e^{Z_l} e^{c} }$$\n",
    "\n",
    "$$ = \\frac{ e^{c} e^{Z_m}}{ e^{c} \\sum_{l=0}^{9} e^{Z_l} }$$\n",
    "\n",
    "$$ f_m(X) = \\mathrm{Pr}(Y = m | X) = \\frac{ e^{Z_m}}{ \\sum_{l=0}^{9} e^{Z_l} }$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** In (4.13), show that if we add constants $c_j$, $j = 0, 1, ..., p$, to each of the corresponding coefficients for each of the classes, then the predictions at any new point $x$ are unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original formula:\n",
    "\n",
    "$$\n",
    "\\mathrm{Pr}(Y = k | X = x) = \\frac{ e^{\\beta_{k0} + \\beta_{k1x}x_1 + ... + \\beta_{kp}x_p} }{\\sum_{l=1}^K e^{\\beta_{k0} + \\beta_{k1x}x_1 + ... + \\beta_{kp}x_p} }\n",
    "$$\n",
    "\n",
    "Adding $c_j$ to each of the coefficients $ \\beta_0, ..., \\beta_p $\n",
    "\n",
    "$$\n",
    "= \\frac{ e^{\\beta_{k0} + c_0 + (\\beta_{k1x} + c_1) x_1 ... + (\\beta_{kp} + c_p) x_p} }{\\sum_{l=1}^K e^{\\beta_{k0} + c_0 + (\\beta_{k1x} + c_1) x_1 ... + (\\beta_{kp} + c_p) x_p} }\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{ e^{\\beta_{k0} + \\beta_{k1x}x_1 + ... + \\beta_{kp}x_p + c_0 + c_1 x_1 + ... + c_p x_p} }{\\sum_{l=1}^K e^{\\beta_{k0} + \\beta_{k1x}x_1 + ... + \\beta_{kp}x_p + c_0 + c_1 x_1 + ... + c_p x_p} }\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{ e^{\\beta_{k0} + \\beta_{k1x}x_1 + ... + \\beta_{kp}x_p} e^{c_0 + c_1 x_1 + ... + c_p x_p} }{\\sum_{l=1}^K e^{\\beta_{k0} + \\beta_{k1x}x_1 + ... + \\beta_{kp}x_p} e^{c_0 + c_1 x_1 + ... + c_p x_p} }\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{ e^{c_0 + c_1 x_1 + ... + c_p x_p} e^{\\beta_{k0} + \\beta_{k1x}x_1 + ... + \\beta_{kp}x_p} }{e^{c_0 + c_1 x_1 + ... + c_p x_p} \\sum_{l=1}^K e^{\\beta_{k0} + \\beta_{k1x}x_1 + ... + \\beta_{kp}x_p} }\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathrm{Pr}(Y = k | X = x) = \\frac{ e^{\\beta_{k0} + \\beta_{k1x}x_1 + ... + \\beta_{kp}x_p} }{\\sum_{l=1}^K e^{\\beta_{k0} + \\beta_{k1x}x_1 + ... + \\beta_{kp}x_p} }\n",
    "$$\n",
    "\n",
    "This shows that the softmax function is *over-parametrized*. However, regularization and SGD typically constrain the solutions so that this is not a problem.\n",
    "\n",
    "* In other words, there are redundant parameters in the sense ethat infinitely many sets of parameters that lead to the same probability predictions, i.e. excess degrees of freedom.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "\n",
    "Show that the negative multinomial log-likelihood (10.14) is equivalent to the negative log of the likelihood expression (4.5) when there are M = 2 classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial log-likelihood:\n",
    "\n",
    "$$\n",
    "- \\sum_{i=1}^n \\sum_{m=0}^{M-1} y_{im} \\mathrm{log} \\left( f_m(x_i) \\right)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "f_m(x_i) = \\mathrm{Pr}(Y = m | X) = \\frac{e^{Z_m}}{\\sum_{l=0}^{M-1} e^{Z_l}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $ M = 2, m = 0, 1$\n",
    "\n",
    "$$\n",
    "- \\sum_{i=1}^n \\sum_{m=0}^1 y_{im} \\mathrm{log} \\left( f_m(x_i) \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "- \\sum_{i=1}^n y_{i1} \\mathrm{log} \\left( f_1(x_i) \\right) + y_{i0} \\mathrm{log} \\left( f_0(x_i) \\right)\n",
    "$$\n",
    "\n",
    "$ y_{im} $ is a binary indicator equal to 1 if the observation's \"ground truth\" value is $m$. Since $m$ can only be 0 or 1, we can optionally represent this an exponent, by brining it into the log expression. \n",
    "\n",
    "$$\n",
    "- \\sum_{i=1}^n  \\mathrm{log} \\left( f_1(x_i)^{y_{i1}} \\right) + \\mathrm{log} \\left( f_0(x_i)^{y_{i0}} \\right)\n",
    "$$\n",
    "\n",
    "Moving the log to the outside\n",
    "\n",
    "$$\n",
    "- \\mathrm{log} \\left[ \\prod_{i=1}^n f_1(x_i)^{y_{i1}} f_0(x_i)^{y_{i0}} \\right]\n",
    "$$\n",
    "\n",
    "In this case, $ f_1(x_i) = \\mathrm{Pr}(Y = 1 | X) = p(x) $.\n",
    "\n",
    "Since there are only two outcomes, 0 and 1, $ f_0(x_i) = \\mathrm{Pr}(Y = 0 | X) = 1 - p(x) $.\n",
    "\n",
    "$$\n",
    "- \\mathrm{log} \\left[ \\prod_{i=1}^n  p(x_i)^{y_{i1}} \\left( 1 - p(x_i)\\right)^{y_{i0}} \\right]\n",
    "$$\n",
    "\n",
    "The similar substitution can be made for $y_{im}$: $y_{i1} = 1 - y_{i0}$\n",
    "\n",
    "$$\n",
    "- \\mathrm{log} \\left[ \\prod_{i=1}^n  p(x_i)^{y_{i}} \\left( 1 - p(x_i)\\right)^{1 - y_{i}} \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\n",
    "\n",
    "Consider a CNN that takes in 32 $\\times$ 32 grayscale images and has a single convolution layer with three 5 $\\times$ 5 convolution filters (without boundary padding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Draw a sketch of the input and first hidden layer similar to Figure 10.8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](Ch10_CNN.png)\n",
    "<!-- <img src=\"Ch10_CNN.png\" width=\"1000\"/> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** How many parameters are in this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution filter: 5 x 5 = 25\n",
    "\n",
    "The filter is applied 28 x 28 = 784 times per layer, however the same set of 25 weights is used, so there are no additional paremters to estimate.\n",
    "\n",
    "The convolutional filter is applied in 3 layers: 25 x 3 = 75 parameters in total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Explain how this model can be thought of as an ordinary feed-forward neural network with the individual pixels as inputs, and with constraints on the weights in the hidden units. What are the constraints?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN can be thought of as an equivalent feed-forward network with:\n",
    "\n",
    "* An input layer of 32 x 32 = 1024 nodes\n",
    "\n",
    "* The hidden layer is the output of the convolution layer, 28 x 28, multipled by 3 filters: 28 x 28 x 3 = 2352 nodes\n",
    "\n",
    "* However, there are two constraints:\n",
    "\n",
    "    * Each of the three 28 x 28 filter layers is created by the same 5 x 5 filter, i.e. the shares the same 25 weights.\n",
    "\n",
    "    * In the output layer, pixels only interact with those within the spatial span of the convolution filter. I.e. pixels in the upper-left corner never interact with pixels in the lower-right corner, as the convolution filter is onl 5 x 5.\n",
    "\n",
    "* Ultimately, the same number of unique parameters are estimated for the feed-forward network: 5 x 5 x 3 = 75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** If there were no constraints, then how many weights would there be in the ordinary feed-forward neural network in (c)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If each applied convolution used a unique set of weights:\n",
    "\n",
    "* Image size (32 x 32) x filter size (5 x 5) x number of filter layers (3) = 1024 x 25 x 3 = 76,800 weights.\n",
    "\n",
    "Additionally, if we think of the output layer as being 28 x 28 x 3 - 2352 and every pixel maps to every node in the hidden layer:\n",
    "\n",
    "* Image size (32 x 32) x hidden layer (28 x 28 x 3) = 1024 x 2352 = 291,648 weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\n",
    "\n",
    "In Table 10.2 on page 426, we see that the ordering of the three methods with respect to mean absolute error is different from the ordering with respect to test set $R^2$. How can this be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R^2$ can be thought of as the proportion of variance explained by the model; whereas mean absolute error is a measure of how similar one set of predictions is from another.\n",
    "\n",
    "A measure of variance is not quite the same as a measure of error. For example, for a random variable $X$ and a constant $a$, $ \\mathrm{Var}(X + a) = \\mathrm{Var}(X) $. That is, a prediction (or estimate) can be translated by a constant, and still have the same variance.\n",
    "\n",
    "However, adding a factor $a$ to a prediction (or estimate), does affect the mean absoute error, increasing the error by the magnitude of $a$: MAE = | X - (X + a) | = | a |\n",
    "\n",
    "In solving problems of prediction, we typically care more about error than variance. Minimizing error will also minimize variance, but minimizing variance will not necessarily minimize error."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
