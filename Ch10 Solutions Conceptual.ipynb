{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10 - Excercise Solutions - Conceptual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "\n",
    "Consider a neural network with two hidden layers: *p* = 4 input units, 2 units in the first hidden layer, 3 units in the second hidden layer, and a single output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Draw a picture of the network, similar to Figures 10.1 or 10.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](Ch10_NN_4x2x3x1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Write out an expression for f(X), assuming ReLU activation functions. Be as explicit as you can!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first hidden layer:\n",
    "\n",
    "$$ A_k^{(1)} = h_k^{(1)} (X) $$\n",
    "\n",
    "$$ A_k^{(1)} = g \\left( w_{k0}^{(1)} + \\sum_{j=1}^{4} w_{kj}^{(1)} X_j \\right) $$\n",
    "\n",
    "for $ k = 1, 2 $\n",
    "\n",
    "$ \\mathbf{W_1} $ has dimensions: (1 + 4) $ \\times $ 2, i.e. **10** elemenets\n",
    "\n",
    "[`1 + seq_length` $\\times$ `num_features_l1`]\n",
    "\n",
    "The second hidden layer:\n",
    "\n",
    "$$ A_l^{(2)} = h_l^{(2)} (X) $$\n",
    "\n",
    "$$ A_l^{(2)} = g \\left( w_{l0}^{(2)} + \\sum_{l=1}^{2} w_{lk}^{(2)} A_k^{(1)} \\right) $$\n",
    "\n",
    "for $ l = 1, 2, 3 $ (differentiated from $ k $ for clarity)\n",
    "\n",
    "$ \\mathbf{W_2} $ has dimensions: (1 + 2) $ \\times $ 3, i.e. **9** elements\n",
    "\n",
    "[`1 + num_features_l1` $\\times$ `num_features_l2`]\n",
    "\n",
    "$g(z)$ is the ReLU (rectified linear unit) function, defined as:\n",
    "\n",
    "$$\n",
    "g(z) =\n",
    "\\begin{cases} \n",
    "      0 & \\text{if } z < 0 \\\\\n",
    "      z & \\text{if } z \\geq 0 \n",
    "   \\end{cases}\n",
    "$$\n",
    "\n",
    "The output layer:\n",
    "\n",
    "$$ f_0(X) = \\beta_0 + \\sum_{l=1}^{3} \\beta_l h_l^{(2)}(X) $$\n",
    "\n",
    "$ \\mathbf{B} $ has dimensions: (1 + 3) $ \\times $ 1, i.e. **4** elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the layers:\n",
    "\n",
    "$$ f_0(X) = \\beta_0 + \\sum_{l=1}^{3} \\beta_l \\; \\; g \\left[ w_{l0}^{(2)} + \\sum_{l=1}^{2} w_{lk}^{(2)} A_k^{(1)} \\right] $$\n",
    "\n",
    "$$ f_0(X) = \\beta_0 + \\sum_{l=1}^{3} \\beta_l \\; \\; g \\left[ w_{l0}^{(2)} + \\sum_{l=1}^{2} w_{lk}^{(2)} \\; g \\left( w_{k0}^{(1)} + \\sum_{j=1}^{4} w_{kj}^{(1)} X_j \\right)   \\right] $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Now plug in some values for the coefficients and write out the value of f(X)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.1607552059888944\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.random.uniform(-10, 10, 4)\n",
    "\n",
    "np.random.seed(1)\n",
    "A1k1 = np.random.uniform(-1, 0, 4) # for k = 1\n",
    "A11 = np.random.uniform(-1, 1) + X.dot(A1k1) # 1 + 4 parameters\n",
    "\n",
    "np.random.seed(2)\n",
    "A1k2 = np.random.uniform(-1, 0, 4) # for k = 2\n",
    "A12 = np.random.uniform(-1, 1) + X.dot(A1k1) # 1 + 4 parameters\n",
    "\n",
    "A1 = abs(np.array([A11, A12])) # ReLU layer\n",
    "\n",
    "np.random.seed(3)\n",
    "A2k1 = np.random.uniform(-1, 0, 2) # for l = 1\n",
    "A21 = np.random.uniform(-1, 1) + A1.dot(A2k1) # 1 + 2 parameters\n",
    "\n",
    "np.random.seed(4)\n",
    "A2k2 = np.random.uniform(-1, 0, 2) # for l = 2\n",
    "A22 = np.random.uniform(-1, 1) + A1.dot(A2k2) # 1 + 2 parameters\n",
    "\n",
    "np.random.seed(5)\n",
    "A2k3 = np.random.uniform(-1, 0, 2) # for l = 3\n",
    "A23 = np.random.uniform(-1, 1) + A1.dot(A2k3) # 1 + 2 parameters\n",
    "\n",
    "A2 = abs(np.array([A21, A22, A23])) # ReLU layer\n",
    "\n",
    "np.random.seed(6)\n",
    "B1 = np.random.uniform(-1, 0, 3) # length 3 for size of prev layer\n",
    "B = np.random.uniform(-1, 1) + A2.dot(B1) # 1 + 3 parameters\n",
    "# No ReLU for output layer\n",
    "print(B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** How many parameters are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 23 parameters in total:\n",
    "\n",
    "* First hidden layer: (1 + 4) $ \\times $ 2, i.e. **10** elemenets\n",
    "\n",
    "* Second hidden layer: (1 + 2) $ \\times $ 3, i.e. **9** elements\n",
    "\n",
    "* Output layer: (1 + 3) $ \\times $ 1, i.e. **4** elements\n",
    "\n",
    "10 + 9 + 4 = 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n",
    "Consider the *softmax* function in (10.13) (see also (4.13) on page 145) for modeling multinomial probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** In (10.13), show that if we add a constant $c$ to each of the $Z_l$, then the probability is unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original formula:\n",
    "\n",
    "$$ f_m(X) = \\mathrm{Pr}(Y = m | X) = \\frac{e^{Z_m}}{\\sum_{l=0}^{9} e^{Z_l} }$$\n",
    "\n",
    "Adding $c$ to each $Z_L$\n",
    "\n",
    "$$ = \\frac{e^{Z_m + c}}{\\sum_{l=0}^{9} e^{Z_l + c} }$$\n",
    "\n",
    "$$ = \\frac{e^{Z_m} e^{c}}{\\sum_{l=0}^{9} e^{Z_l} e^{c} }$$\n",
    "\n",
    "$$ = \\frac{ e^{c} e^{Z_m}}{ e^{c} \\sum_{l=0}^{9} e^{Z_l} }$$\n",
    "\n",
    "$$ f_m(X) = \\mathrm{Pr}(Y = m | X) = \\frac{ e^{Z_m}}{ \\sum_{l=0}^{9} e^{Z_l} }$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** In (4.13), show that if we add constants $c_j$, $j = 0, 1, ..., p$, to each of the corresponding coefficients for each of the classes, then the predictions at any new point $x$ are unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original formula:\n",
    "\n",
    "$$\n",
    "\\mathrm{Pr}(Y = k | X = x) = \\frac{ e^{\\beta_{k0} + \\beta_{k1x}x_1 + ... + \\beta_{kp}x_p} }{\\sum_{l=1}^K e^{\\beta_{k0} + \\beta_{k1x}x_1 + ... + \\beta_{kp}x_p} }\n",
    "$$\n",
    "\n",
    "Adding $c_j$ to each of the coefficients $ \\beta_0, ..., \\beta_p $\n",
    "\n",
    "$$\n",
    "= \\frac{ e^{\\beta_{k0} + c_0 + (\\beta_{k1x} + c_1) x_1 ... + (\\beta_{kp} + c_p) x_p} }{\\sum_{l=1}^K e^{\\beta_{k0} + c_0 + (\\beta_{k1x} + c_1) x_1 ... + (\\beta_{kp} + c_p) x_p} }\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{ e^{\\beta_{k0} + \\beta_{k1x}x_1 + ... + \\beta_{kp}x_p + c_0 + c_1 x_1 + ... + c_p x_p} }{\\sum_{l=1}^K e^{\\beta_{k0} + \\beta_{k1x}x_1 + ... + \\beta_{kp}x_p + c_0 + c_1 x_1 + ... + c_p x_p} }\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{ e^{\\beta_{k0} + \\beta_{k1x}x_1 + ... + \\beta_{kp}x_p} e^{c_0 + c_1 x_1 + ... + c_p x_p} }{\\sum_{l=1}^K e^{\\beta_{k0} + \\beta_{k1x}x_1 + ... + \\beta_{kp}x_p} e^{c_0 + c_1 x_1 + ... + c_p x_p} }\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{ e^{c_0 + c_1 x_1 + ... + c_p x_p} e^{\\beta_{k0} + \\beta_{k1x}x_1 + ... + \\beta_{kp}x_p} }{e^{c_0 + c_1 x_1 + ... + c_p x_p} \\sum_{l=1}^K e^{\\beta_{k0} + \\beta_{k1x}x_1 + ... + \\beta_{kp}x_p} }\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathrm{Pr}(Y = k | X = x) = \\frac{ e^{\\beta_{k0} + \\beta_{k1x}x_1 + ... + \\beta_{kp}x_p} }{\\sum_{l=1}^K e^{\\beta_{k0} + \\beta_{k1x}x_1 + ... + \\beta_{kp}x_p} }\n",
    "$$\n",
    "\n",
    "This shows that the softmax function is *over-parametrized*. However, regularization and SGD typically constrain the solutions so that this is not a problem.\n",
    "\n",
    "* In other words, there are redundant parameters in the sense ethat infinitely many sets of parameters that lead to the same probability predictions, i.e. excess degrees of freedom.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "\n",
    "Show that the negative multinomial log-likelihood (10.14) is equivalent to the negative log of the likelihood expression (4.5) when there are M = 2 classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial log-likelihood:\n",
    "\n",
    "$$\n",
    "- \\sum_{i=1}^n \\sum_{m=0}^M y_{im} \\mathrm{log} \\left( f_m(x_i) \\right)\n",
    "$$\n",
    "\n",
    "For $ M = 2, m = 0, 1$\n",
    "\n",
    "$$\n",
    "- \\sum_{i=1}^n \\sum_{m=0}^1 y_{im} \\mathrm{log} \\left( f_m(x_i) \\right)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
